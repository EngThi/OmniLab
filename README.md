# üåê OmniLab

> Interface multimodal estilo Laborat√≥rio Stark ‚Äî autossuficiente, modular e escal√°vel.

**Status:** Desenvolvimento ativo ¬∑ [Hackatime/Flavortown](https://flavortown.hackclub.com)

---

## O que √©?

OmniLab √© uma plataforma de intera√ß√£o multimodal que combina:

- üëÅÔ∏è **Vis√£o Computacional** ‚Äî gestos de m√£o detectados pela c√¢mera
- üó£Ô∏è **Voz** ‚Äî comandos de linguagem natural
- üß† **IA Orquestradora** ‚Äî JARVIS-like, processa inten√ß√£o e despacha a√ß√µes
- üñ•Ô∏è **HUD 3D no Browser** ‚Äî interface hologr√°fica renderizada em Three.js

Processamento pesado roda na nuvem. O m√≥dulo local s√≥ √© necess√°rio para acessar a c√¢mera.

---

## Setup Local

> S√≥ necess√°rio quando for usar a c√¢mera. Nada de depend√™ncia suja no seu PC.

```bash
# 1. Clone o repo
git clone https://github.com/EngThi/OmniLab.git
cd OmniLab

# 2. Crie o ambiente virtual (fica s√≥ aqui, n√£o vai pro Git)
python -m venv .venv
source .venv/bin/activate  # Linux/Mac

# 3. Instale as depend√™ncias
pip install -r requirements.txt

# 4. Baixe o modelo MediaPipe (n√£o vai pro Git)
python scripts/download_model.py

# 5. Rode o m√≥dulo de vis√£o
python vision.py
```

---

## Estrutura

```
OmniLab/
‚îú‚îÄ‚îÄ vision.py              # M√≥dulo de c√¢mera: detecta gestos, envia JSON
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias Python (s√≥ m√≥dulo local)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ download_model.py  # Baixa o hand_landmarker.task (~7.8MB)
‚îî‚îÄ‚îÄ README.md
```

> ‚ö†Ô∏è O modelo `hand_landmarker.task` e o `.venv/` **n√£o est√£o no repo** ‚Äî s√£o gerados localmente via scripts acima.
